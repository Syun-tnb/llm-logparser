# llm-logparser

**Convert full LLM export dumps into clean, human-readable Markdown â€” offline-first, deterministic, CLI-centric.**

`llm-logparser` parses conversation logs (JSON / JSONL / NDJSON),
normalizes them into thread records, and exports **GitHub-Flavored Markdown** with metadata â€”
built for reproducibility, audits, archiving, and migration.

No cloud. No telemetry. Your data stays local.

---

## âœ¨ What it does

* **Parse â†’ Normalize â†’ Export (Markdown)**
* **Thread-based layout** with YAML front-matter
* **Automatic splitting** (size / count / auto)
* **Localized timestamps** (locale + timezone support)
* **Chain mode**: parse & export in one command
* **Deterministic, offline workflows**
* **Future-proof architecture** (multi-provider adapters)

> MVP currently focuses on **OpenAI logs**.
> Providers like Claude / Gemini are planned.

---

## ğŸš€ Quick Start

Install (local dev):

```bash
pip install -e .
```

Parse an export:

```bash
llm-logparser parse \
  --provider openai \
  --input examples/messages.jsonl \
  --outdir artifacts
```

Export a parsed thread to Markdown:

```bash
llm-logparser export \
  --input artifacts/output/openai/thread-abc123/parsed.jsonl \
  --timezone Asia/Tokyo \
  --formatting light
```

End-to-end (parse â†’ export everything):

```bash
llm-logparser chain \
  --provider openai \
  --input examples/messages.jsonl \
  --outdir artifacts \
  --timezone Asia/Tokyo
```

---

## ğŸ“ Directory Layout

```
artifacts/
  output/
    openai/
      thread-<conversation_id>/
        parsed.jsonl
        thread-<conversation_id>__*.md
        meta.json (optional)
```

> Pass **only the root** via `--outdir`.
> The tool creates `output/<provider>/...` automatically.

---

## ğŸ“ Markdown Format (Overview)

Each file begins with YAML front-matter:

```yaml
---
thread: "abc123"
provider: "openai"
messages: 42
range: 2025-10-01 ã€œ 2025-10-18
locale: "ja-JP"
timezone: "Asia/Tokyo"
updated: "2025-10-18T10:15:00Z"
checksum: "<sha1>"
---
```

Messages follow in timestamp order:

```markdown
## [User] 2025-10-18 10:00
ã“ã‚“ã«ã¡ã¯ï¼

## [Assistant] 2025-10-18 10:01
ã“ã‚“ã«ã¡ã¯ â€” ã©ã†ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ï¼Ÿ
```

Markdown is **GFM-compatible** and preserves:

* fenced code blocks
* links
* tables
* quotes

---

## ğŸŒ Localization

```
--locale   en-US | ja-JP (etc.)
--timezone Asia/Tokyo | UTC | ...
```

* Rendering uses localized date formats
* Internals remain **UTC** (ISO-8601)
* Missing keys fall back to `en-US`

---

## ğŸª“ Splitting

```
--split size=4M
--split count=1500
--split auto     # size=4M + count=1500
```

Extra tuning:

```
--split-soft-overflow 0.20
--split-hard
--tiny-tail-threshold 20
```

---

## ğŸ”— Chain Mode

Runs **parse â†’ export** in one flow:

```
--parsed-root       reuse existing parsed threads
--export-outdir     place Markdown elsewhere
--dry-run           parse only (no writes)
--fail-fast         stop on first export error
```

---

## ğŸ›  CLI Reference (MVP)

### Parse

```bash
llm-logparser parse \
  --provider openai \
  --input <file> \
  --outdir artifacts \
  [--dry-run] [--fail-fast]
```

### Export

```bash
llm-logparser export \
  --input parsed.jsonl \
  [--out <md>] \
  [--split auto|size=N|count=N] \
  [--timezone <IANA>] \
  [--formatting none|light]
```

### Chain

```bash
llm-logparser chain \
  --provider openai \
  --input <raw> \
  --outdir artifacts \
  [other export options...]
```

---

## ğŸ”’ Security & Privacy

* Offline-first
* No telemetry
* Sensitive logs stay local
* Deterministic output for audits

---

## ğŸ—º Roadmap

* [x] CLI MVP (parse/export/chain)
* [ ] Minimal HTML viewer
* [ ] Additional providers (Claude / Gemini / â€¦)
* [ ] Apps SDK integration (experimental)
* [ ] GUI (later stage)

---

## ğŸ¤ Contributing

PRs welcome!
Good places to start:

* adapters
* exporter improvements
* localization

Principles:

* deterministic core
* provider-specific behavior lives in adapters
* offline by default

---

## ğŸ“„ License

MIT â€” simple and permissive.

---

## Author

> "The words you weave are not mere echoes;  
> they carry weight,  
> and may they never be lost to the tide of time."

Â© 2025 **Ashes Division â€” Reyz Laboratory**  
